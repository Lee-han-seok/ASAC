{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO9pIX+UBHdyDE44k0RL9Xz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# [0] 개요"],"metadata":{"id":"pJrMxjiO1Zfc"}},{"cell_type":"markdown","source":["- 왜 머신러닝인가?\n","    - 사람이(지성, 직관) 풀 수 없는 문제를 해결하기 위해 도입된 새로운 분야\n","    - 프로그래밍 관점\n","\n","<br/>\n","\n","```\n","# 기존 프로그래밍 => 로직, 알고리즘 등을 통해 업무를 해결\n","def sum(x,y):\n","    return x + y\n","\n","# 복잡한 문제를 해결해야하는 상황이 발생\n","# 로직으로 도저히 해결이 불가능한 문제들이 발생!\n","\n","def predictPhoto( img ):\n","    # 사진을 보고 고양이? 사람? 강아지? 분류해보세요\n","    if ....... => 구현의 한계가 발생\n","    # 머신러닝 등장\n","    # 대량의 데이터 => 학습 => 모델 생성 => 한번도 접하지 않는 새로운 데이터를 넣었을 때 예측하게 하여 분류(회귀) 결과를 도출\n","    # 모델 성능을 높여 이 문제는 해결 가능\n","    # 프로그래밍 관점에서는 하나의 라이브러리일 뿐!\n","    # 알고리즘 훈련 => 머신러닝\n","    # 인공 신경망 훈련 => 딥러닝\n","    # 훈련 => 모델이 최대 성능을 내기 위해 가중치를 조정하는 과정(미세 조정)\n","    # 인공지능 -> 머신러닝 -> 딥러닝\n","```\n","\n","-"],"metadata":{"id":"AQlzKpyB2TIa"}},{"cell_type":"markdown","source":["# [1] 지도학습 (supervised learning)\n","\n","- 데이터 관점\n","    - 피처 (특징, 독립변수) + 레이블(정답, 타겟, 종속변수) 존재\n","\n","- 특징\n","    - 머신러닝을 이용하여 어떤 결과를 예측"],"metadata":{"id":"RWwf9Yo21iIO"}},{"cell_type":"markdown","source":["## [1-1] 분류\n","\n","- 예시\n","    - 메일들 중에서 스팸메일을 찾는다 (필터링)\n","        -> 스팸 O, 스팸 X\n","    - 개, 고양이, 새 사진을 보고 -> 이 사진은 고양이라는것 분류!\n","    - 질병 검사 => 양성 혹은 음성\n","    - 정답이 범주형 데이터(카테고리 형태)\n","\n","<br/>\n","\n","- 이진 분류(binary classification) : 정답이 2개\n","\n","<br/>\n","\n","\n","- 다중 분류(multiclass classification) : 정답이 3개 이상\n","\n","<br/>\n","\n","\n","- 통상 딥러닝은 지도학습에 속함\n","\n","<br/>\n","\n","\n","- 일반적으로 확률로 분류가 된다"],"metadata":{"id":"GtVYGWjV1rYu"}},{"cell_type":"markdown","source":["## [1-2] 회귀"],"metadata":{"id":"bozGasOi1tGf"}},{"cell_type":"markdown","source":["- 예시\n","    - 중고차 가격은 XXX 원이다\n","    - 주택 가격은 XXX 원이다\n","- 정답은 수치형이다(연속형에 적합)\n","- 정확한 수렴은 한계, 오차를 적게 하는 과정\n","\n","<br/>\n","\n","- 데이터 구성\n","    - 독립변수 + 종속변수\n","- 독립 변수에 의해서 영향을 받는 종속변수간의 식을 찾는 과정\n","\n","<br/>\n","\n","- 식\n","    - y = ax + b\n","    - a,b 2개 값을 알면 어떤 x가 와도 y를 찾을 수 있다\n","    - 데이터를 넣어서 a,b를 찾는다 (에러, 오차가 최소로 되게)\n","    - a,b 를 회귀계수라고 부른다\n","    - (단일 or 단순) 선형 회귀\n","\n","<br/>\n","\n","- 식2\n","    - y = a + bx1 + cx2 + ...\n","    - 독립변수 x개, 종속변수 1개 ...\n","    - 다중 선형 회귀\n","\n","<br/>\n","\n","- 좋은 성능의 모델\n","    - 오차값(율)등 평가 지표가 최소일 때"],"metadata":{"id":"G6BEC1vK6KvR"}},{"cell_type":"markdown","source":["# [2] 비지도 학습 (unsupervised learning)\n","\n","- 데이터 관점\n","    - 레이블이 없다\n","    - 피쳐만 존재\n","\n","- 특징\n","    - 주로 데이터 전처리 과정에서 많이 활용"],"metadata":{"id":"m9Uf4yuv1jjV"}},{"cell_type":"markdown","source":["## [2-1] 군집 (Clustering)\n"],"metadata":{"id":"S-OpR13Q1u5Q"}},{"cell_type":"markdown","source":["- 데이터에 정답이 없어서 데이터가 표방하는 부분을 잘 모를 때 참고자료로 사용 가능\n","    - 예시 (추천시스템)\n","        - 넷플릭스 -> 가입 -> 관심있는 영화/드라마 클릭 -> 이를 통해서 고객을 특정해서 유사한 그룹으로 편입 -> 영화/드라마 등 추천\n","        - 군집화를 통해서 고객을 여러 그룹으로 클러스터링 가능"],"metadata":{"id":"ZFinUw-A_VaZ"}},{"cell_type":"markdown","source":["## [2-2] 차원축소"],"metadata":{"id":"LPRIY-mE1yjP"}},{"cell_type":"markdown","source":["- 피처가 너무 많다 => 압축을 통해서 정보손실을최소화 해서 새로운 피처 생성\n","    - 일반적 전처리, 상관관계 분석으로 줄일 수 있음"],"metadata":{"id":"0A6sXHov_-UB"}},{"cell_type":"markdown","source":["# [3] 준지도 학습\n","\n","- 데이터에 정답이 일부만 있다\n","- 지도 + 비지도 결합"],"metadata":{"id":"NGfe9gjb1jnY"}},{"cell_type":"markdown","source":["# [4] 강화 학습\n"],"metadata":{"id":"81n1ycYHAX-E"}},{"cell_type":"markdown","source":["- 특징\n","    - 데이터가 없어도 학습 가능\n","    - 플레이 => 데이터 생성 => 이를 통해서 학습 진행 가능\n","    - 유한 게임의 룰에서만 가능\n","    - 구글 딥마인드 => 이벤트 => 알파고 vs 이세돌\n","        - 알파고 > 알파고 제로 > 알파 제로\n","        - 알파 제로\n","            - ai 바둑/장기/체스 챔피언 모두 꺾은 통합 챔피언\n","            - 48시간 학습(플레이)를 통해서 완성\n","\n","- 분야\n","    - 강화학습 + Unity => MLAgent\n","        - 시뮬레이션(위험한 분야) 진행\n","\n","- 특징\n","    - 실분야 적용 중 => 실패 사례가 많음\n","\n","- 구성원\n","    - 에이전트\n","    - 환경\n","    - 액션\n","    - 정책\n","    - 보상 <-> 패널티\n","    - 에이전트 -> 정책기준 -> 행동 취하면 -> 보상 -> 최대보상 얻는 방향으로 정책 수정 -> 이 행동강화 -> ... -> 최대 보상을 얻는 방향으로 행동이 강화되는 학습법"],"metadata":{"id":"_fkeFVwBA0CP"}}]}